{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MyOriginalPytorchLSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xa5kNDOT1Y-G"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from pickle import dump, load\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JkkE7tn1tp3"
      },
      "source": [
        "mm = preprocessing.MinMaxScaler() # min max scaler\n",
        "ss = preprocessing.StandardScaler() #standard scaler\n",
        "\n",
        "def load_data():\n",
        "    data_dir = '/content/drive/MyDrive/Fibre Laser/'\n",
        "    files = sorted(os.listdir(data_dir), key=len)\n",
        "    files.remove('target.npz') \n",
        "    files.remove('FibreLaserLSTM.pt')\n",
        "    files.remove('MinMaxScaler.pkl')\n",
        "\n",
        "    X = np.empty((0, 16384))\n",
        "    y = np.empty((0, 5))\n",
        "\n",
        "    for f in files:\n",
        "        data = np.load(data_dir + f)\n",
        "        X = np.append(X, np.array(data.get(\"output_spectrum\")), axis=0).astype(np.float32)\n",
        "        y = np.append(y, np.array(data.get(\"laser_parameters\")), axis=0).astype(np.float32)\n",
        "\n",
        "    nan_rows = ~np.isnan(X).any(axis=1)\n",
        "\n",
        "    X = X[nan_rows]\n",
        "    y = y[nan_rows]\n",
        "    \n",
        "    ss.fit(X)\n",
        "    mm.fit(y)\n",
        "\n",
        "    X = ss.transform(X)\n",
        "    y = mm.transform(y)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state=42)\n",
        "\n",
        "    X_train = torch.from_numpy(X_train)\n",
        "    y_train = torch.from_numpy(y_train)\n",
        "    X_test = torch.from_numpy(X_test)\n",
        "    y_test = torch.from_numpy(y_test)\n",
        "\n",
        "    return(X_train, X_test, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAqJKm1ouf7L"
      },
      "source": [
        "def load_target():\n",
        "  data_dir = '/content/drive/MyDrive/Fibre Laser/'\n",
        "  f = 'target.npz'\n",
        "  target = np.load(data_dir + f)['target_spectrum'].astype(np.float32)\n",
        "  target = torch.from_numpy(target)\n",
        "  return target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnAtSBvj2OGb"
      },
      "source": [
        "input_dim = 16384 # represents the size of the input at each time step, e.g. input of dimension 5 will look like this [1, 3, 8, 2, 3]\n",
        "hidden_dim = 256 # represents the size of the hidden state and cell state at each time step 256\n",
        "num_layers = 2 # the number of LSTM layers stacked on top of each other 2\n",
        "num_outputs = 5 # 5 parameters to predict\n",
        "\n",
        "batch_size = 48\n",
        "sequence_length = 1\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_outputs, dropout=0.3):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm1 = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first = True)\n",
        "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.dout = nn.Dropout(p=0.2)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_outputs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden_state = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
        "        cell_state = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
        "\n",
        "        out, hidden = self.lstm1(x, (hidden_state, cell_state))\n",
        "        #out = out.reshape(out.shape[0], -1)\n",
        "        out = self.fc1(out[:,-1,:])\n",
        "        out = self.dout(out)\n",
        "        out = self.fc2(out)\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.num_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "                      weight.new(self.num_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
        "        return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpPJ7QTH2SON",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b4f27e8-2884-4a2c-ebcb-ada8d6ea1b63"
      },
      "source": [
        "X, X_test, y, y_test = load_data()\n",
        "\n",
        "X = X.reshape(-1, sequence_length, input_dim)\n",
        "X_test = X_test.reshape(-1, sequence_length, input_dim)\n",
        "print(X.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([6428, 1, 16384])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBB-vChvP1DX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0835ea5e-a3d9-4557-f9b5-bc3efdb26c2c"
      },
      "source": [
        "train_data = TensorDataset(X, y)\n",
        "train_loader = DataLoader(dataset = train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "num_epochs = 100 # training epochs 100\n",
        "learning_rate = 0.001 # optimizer lr 0.001\n",
        "\n",
        "model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim,\n",
        "             num_layers=num_layers, num_outputs=num_outputs)\n",
        "model.to(device)\n",
        "\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    total_loss = 0.0\n",
        "    bath_num = 0\n",
        "    for batch_index, (data, labels) in enumerate(train_loader):\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        y_pred, hidden = model(data)\n",
        "\n",
        "        loss = criterion(y_pred, labels)\n",
        "        total_loss+=loss\n",
        "        bath_num = batch_index\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(\"| Epoch:\", epoch, \"| Average Loss:\", total_loss/bath_num, \"|\")\n",
        "    \n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Epoch: 0 | Average Loss: tensor(0.0214, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 1 | Average Loss: tensor(0.0081, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 2 | Average Loss: tensor(0.0072, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 3 | Average Loss: tensor(0.0067, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 4 | Average Loss: tensor(0.0062, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 5 | Average Loss: tensor(0.0060, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 6 | Average Loss: tensor(0.0059, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 7 | Average Loss: tensor(0.0059, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 8 | Average Loss: tensor(0.0057, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 9 | Average Loss: tensor(0.0055, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 10 | Average Loss: tensor(0.0054, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 11 | Average Loss: tensor(0.0054, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 12 | Average Loss: tensor(0.0053, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 13 | Average Loss: tensor(0.0053, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 14 | Average Loss: tensor(0.0050, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 15 | Average Loss: tensor(0.0050, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 16 | Average Loss: tensor(0.0048, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 17 | Average Loss: tensor(0.0050, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 18 | Average Loss: tensor(0.0050, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 19 | Average Loss: tensor(0.0047, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 20 | Average Loss: tensor(0.0047, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 21 | Average Loss: tensor(0.0047, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 22 | Average Loss: tensor(0.0046, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 23 | Average Loss: tensor(0.0044, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 24 | Average Loss: tensor(0.0045, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 25 | Average Loss: tensor(0.0045, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 26 | Average Loss: tensor(0.0043, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 27 | Average Loss: tensor(0.0044, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 28 | Average Loss: tensor(0.0043, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 29 | Average Loss: tensor(0.0043, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 30 | Average Loss: tensor(0.0042, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 31 | Average Loss: tensor(0.0043, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 32 | Average Loss: tensor(0.0043, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 33 | Average Loss: tensor(0.0042, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 34 | Average Loss: tensor(0.0042, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 35 | Average Loss: tensor(0.0042, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 36 | Average Loss: tensor(0.0041, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 37 | Average Loss: tensor(0.0041, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 38 | Average Loss: tensor(0.0040, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 39 | Average Loss: tensor(0.0040, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 40 | Average Loss: tensor(0.0039, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 41 | Average Loss: tensor(0.0040, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 42 | Average Loss: tensor(0.0039, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 43 | Average Loss: tensor(0.0039, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 44 | Average Loss: tensor(0.0039, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 45 | Average Loss: tensor(0.0039, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 46 | Average Loss: tensor(0.0039, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 47 | Average Loss: tensor(0.0038, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 48 | Average Loss: tensor(0.0039, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 49 | Average Loss: tensor(0.0038, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 50 | Average Loss: tensor(0.0038, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 51 | Average Loss: tensor(0.0038, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 52 | Average Loss: tensor(0.0038, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 53 | Average Loss: tensor(0.0036, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 54 | Average Loss: tensor(0.0037, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 55 | Average Loss: tensor(0.0037, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 56 | Average Loss: tensor(0.0038, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 57 | Average Loss: tensor(0.0037, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 58 | Average Loss: tensor(0.0036, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 59 | Average Loss: tensor(0.0037, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 60 | Average Loss: tensor(0.0036, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 61 | Average Loss: tensor(0.0036, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 62 | Average Loss: tensor(0.0036, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 63 | Average Loss: tensor(0.0035, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 64 | Average Loss: tensor(0.0035, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 65 | Average Loss: tensor(0.0034, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 66 | Average Loss: tensor(0.0034, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 67 | Average Loss: tensor(0.0034, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 68 | Average Loss: tensor(0.0034, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 69 | Average Loss: tensor(0.0033, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 70 | Average Loss: tensor(0.0033, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 71 | Average Loss: tensor(0.0033, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 72 | Average Loss: tensor(0.0034, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 73 | Average Loss: tensor(0.0034, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 74 | Average Loss: tensor(0.0034, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 75 | Average Loss: tensor(0.0033, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 76 | Average Loss: tensor(0.0033, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 77 | Average Loss: tensor(0.0033, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 78 | Average Loss: tensor(0.0032, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 79 | Average Loss: tensor(0.0034, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 80 | Average Loss: tensor(0.0032, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 81 | Average Loss: tensor(0.0033, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 82 | Average Loss: tensor(0.0032, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 83 | Average Loss: tensor(0.0031, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 84 | Average Loss: tensor(0.0032, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 85 | Average Loss: tensor(0.0033, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 86 | Average Loss: tensor(0.0033, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 87 | Average Loss: tensor(0.0032, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 88 | Average Loss: tensor(0.0031, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 89 | Average Loss: tensor(0.0032, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 90 | Average Loss: tensor(0.0034, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 91 | Average Loss: tensor(0.0033, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 92 | Average Loss: tensor(0.0032, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 93 | Average Loss: tensor(0.0032, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 94 | Average Loss: tensor(0.0033, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 95 | Average Loss: tensor(0.0032, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 96 | Average Loss: tensor(0.0032, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 97 | Average Loss: tensor(0.0032, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 98 | Average Loss: tensor(0.0031, grad_fn=<DivBackward0>) |\n",
            "| Epoch: 99 | Average Loss: tensor(0.0032, grad_fn=<DivBackward0>) |\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIQnjhbAdUj_"
      },
      "source": [
        "torch.save(model.state_dict(), '/content/drive/MyDrive/Fibre Laser/FibreLaserLSTM.pt')\n",
        "dump(mm, open('/content/drive/MyDrive/Fibre Laser/MinMaxScaler.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r77vjPSV3Ok_"
      },
      "source": [
        "model_file = '/content/drive/MyDrive/Fibre Laser/FibreLaserLSTM.pt'\n",
        "scaler_file = '/content/drive/MyDrive/Fibre Laser/MinMaxScaler.pkl'\n",
        "if(os.path.exists(model_file) and os.path.exists(scaler_file)):\n",
        "    model = LSTM(input_dim=16384, hidden_dim=256,\n",
        "             num_layers=2, num_outputs=5)\n",
        "    model.to(device)\n",
        "    model.load_state_dict(torch.load(model_file))\n",
        "    model.eval()\n",
        "    criterion = torch.nn.MSELoss()\n",
        "    mm = load(open(scaler_file, 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vS9jVyvVKXzq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77c855f7-a67c-4192-c28e-c9b06b133be1"
      },
      "source": [
        "y_pred, _ = model(X_test.to(device))\n",
        "loss = criterion(y_pred.to(device), y_test.to(device))\n",
        "print(\"Test Loss:\", loss)\n",
        "print(y_pred[:5])\n",
        "print(\"---------\")\n",
        "print(y_test[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: tensor(0.0046, grad_fn=<MseLossBackward>)\n",
            "tensor([[0.6194, 0.5978, 0.6248, 0.7605, 0.3688],\n",
            "        [0.2787, 0.5917, 0.6518, 0.7738, 0.3678],\n",
            "        [0.2787, 0.5917, 0.6518, 0.7738, 0.3678],\n",
            "        [0.4985, 0.6017, 0.6196, 0.7650, 0.3691],\n",
            "        [0.4846, 0.5848, 0.6484, 0.7805, 0.3804]], grad_fn=<SliceBackward>)\n",
            "---------\n",
            "tensor([[0.5384, 0.5842, 0.6511, 0.8352, 0.3738],\n",
            "        [0.2716, 0.6652, 0.6250, 0.7442, 0.3774],\n",
            "        [0.1658, 0.5176, 0.5884, 0.8496, 0.3296],\n",
            "        [0.5713, 0.5676, 0.5713, 0.7938, 0.3762],\n",
            "        [0.3194, 0.4994, 0.6643, 0.7612, 0.3918]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHa6deCTKaZu",
        "outputId": "99acddb6-7498-44a1-c81b-f656e500b718"
      },
      "source": [
        "target = load_target().reshape(-1, sequence_length, input_dim)\n",
        "target_params, _ = model(target.to(device))\n",
        "target_params = mm.inverse_transform(target_params.cpu().detach().numpy())\n",
        "print(\"Predicted Parameters for Target:\", target_params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Parameters for Target: [[ 7.6100249e+03  1.1001486e+00 -1.9183685e+00 -1.2154552e+00\n",
            "   2.1481848e-14]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}