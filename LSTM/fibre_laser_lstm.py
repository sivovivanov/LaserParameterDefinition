# -*- coding: utf-8 -*-
"""MyOriginalPytorchLSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14bNZXXgD30psv1JQr7ZbXcGbnrZElH1W
"""

import os
import time
import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

from sklearn.model_selection import train_test_split
from sklearn import preprocessing

from pickle import dump, load

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device is:", device)

mm = preprocessing.MinMaxScaler() # min max scaler
ss = preprocessing.StandardScaler() #standard scaler

def load_data():
    data_dir = './data/'
    files = sorted(os.listdir(data_dir), key=len)

    X = np.empty((0, 16384))
    y = np.empty((0, 5))

    for f in files:
        data = np.load(data_dir + f)
        X = np.append(X, np.array(data.get("output_spectrum")), axis=0).astype(np.float32)
        y = np.append(y, np.array(data.get("laser_parameters")), axis=0).astype(np.float32)

    nan_rows = ~np.isnan(X).any(axis=1)

    X = X[nan_rows]
    y = y[nan_rows]
    
    ss.fit(X)
    mm.fit(y)

    X = ss.transform(X)
    y = mm.transform(y)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state=42)

    X_train = torch.from_numpy(X_train)
    y_train = torch.from_numpy(y_train)
    X_test = torch.from_numpy(X_test)
    y_test = torch.from_numpy(y_test)

    return(X_train, X_test, y_train, y_test)

def load_target():
  data_dir = './'
  f = 'target.npz'
  target = np.load(data_dir + f)['target_spectrum'].astype(np.float32)
  target = torch.from_numpy(target)
  return target

#--------------------------
# KEY MODEL PARAMETERS
#--------------------------
input_dim = 16384 # represents the size of the input at each time step, e.g. input of dimension 5 will look like this [1, 3, 8, 2, 3]
hidden_dim = 256 # represents the size of the hidden state and cell state at each time step 256
num_layers = 2 # the number of LSTM layers stacked on top of each other 2
num_outputs = 5 # 5 parameters to predict

batch_size = 48
sequence_length = 1

class LSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_layers, num_outputs, dropout=0.3):
        super(LSTM, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        self.lstm1 = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first = True)
        self.fc1 = nn.Linear(hidden_dim, hidden_dim)
        self.dout = nn.Dropout(p=0.2)
        self.fc2 = nn.Linear(hidden_dim, num_outputs)

    def forward(self, x):
        hidden_state = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)
        cell_state = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)

        out, hidden = self.lstm1(x, (hidden_state, cell_state))
        #out = out.reshape(out.shape[0], -1)
        out = self.fc1(out[:,-1,:])
        out = self.dout(out)
        out = self.fc2(out)
        return out, hidden


#--------------------------
# LOADING IN DATA AND GETTING IT IN THE RIGHT SHAPE
#--------------------------
X, X_test, y, y_test = load_data()

X = X.reshape(-1, sequence_length, input_dim)
X_test = X_test.reshape(-1, sequence_length, input_dim)
print("X shape is:", X.shape)

train_data = TensorDataset(X, y)
train_loader = DataLoader(dataset = train_data, batch_size=batch_size, shuffle=True)

#--------------------------
# TRAINING
#--------------------------
num_epochs = 100 # training epochs 100
learning_rate = 0.001 # optimizer lr 0.001

model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim,
             num_layers=num_layers, num_outputs=num_outputs)
model.to(device)

criterion = torch.nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

for epoch in range(num_epochs):
    total_loss = 0.0
    bath_num = 0
    for batch_index, (data, labels) in enumerate(train_loader):
        data, labels = data.to(device), labels.to(device)
        y_pred, _ = model(data)

        loss = criterion(y_pred, labels)
        total_loss+=loss
        bath_num = batch_index
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    print("| Epoch:", epoch, "| Average Loss:", (total_loss/bath_num).item())
    
print("Done")

#--------------------------
# RESET SAVED MODEL/SCALER
#--------------------------
#torch.save(model.state_dict(), './model_save.pt')
#dump(mm, open('./parameter_scaler.pkl', 'wb'))

#--------------------------
# TESTING THE MODEL
#--------------------------
y_pred, _ = model(X_test.to(device))
loss = criterion(y_pred.to(device), y_test.to(device))
print("Test Loss:", loss.item())

print("Predicted values")
print(y_pred[:5])
print("---------")
print("Real values")
print(y_test[:5])
print("---------")

target = load_target().reshape(-1, sequence_length, input_dim)
target_params, _ = model(target.to(device))
target_params = mm.inverse_transform(target_params.cpu().detach().numpy())
print("Predicted Parameters for Target:", target_params)